{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: 20_Newsgroups Multi-class Text Classification\n",
    "## Complete ML Pipeline with Feature Engineering and Selection\n",
    "\n",
    "**Author:** Samidur Rahman  \n",
    "**CMyse:** CMT122 - Machine Learning for NLP  \n",
    "**Academic Year:** 2025/2026\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction & Objectives](#section1)\n",
    "2. [Imports & Environment Setup](#section2)\n",
    "3. [Data Loading & Exploration](#section3)\n",
    "4. [Text Preprocessing](#section4)\n",
    "5. [Dataset Partitioning (Train/Dev/Test)](#section5)\n",
    "6. [Feature Engineering](#section6)\n",
    "   - 6.1. [TF-IDF Features](#section6_1)\n",
    "   - 6.2. [Statistical Text Features](#section6_2)\n",
    "   - 6.3. [Feature Combination](#section6_3)\n",
    "7. [Feature Selection (Development Set)](#section7)\n",
    "8. [Model Selection (Development Set)](#section8)\n",
    "9. [Final Evaluation (Test Set)](#section9)\n",
    "10. [Detailed Performance Analysis](#section10)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction & Objectives <a id='section1'></a>\n",
    "\n",
    "### Objective \n",
    "\n",
    "Implementing a complete machine learning pipeline for **multi-class text classification** on the 20_Newsgroups dataset. The task is to automatically categorise news articles into 6 predefined categories.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "- **SMyce**: Modified 20_Newsgroups dataset\n",
    "- **Size**: 3,416 articles\n",
    "- **Classes**: 6 categories (class-1 through class-6)\n",
    "- **Format**: CSV with columns `text` (article content) and `label` (category)\n",
    "- **Task Type**: Multi-class classification (6 classes)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Environment Setup <a id='section2'></a>\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "**Library Purposes:**\n",
    "- **numpy, pandas**: Data manipulation, array operations, DataFrames\n",
    "- **nltk**: Natural language toolkit for text preprocessing resMyces\n",
    "- **re**: Regular expressions for pattern-based text cleaning\n",
    "- **sklearn**: Complete ML pipeline (preprocessing, models, evaluation)\n",
    "- **scipy.sparse**: Efficient sparse matrix operations for text data\n",
    "\n",
    "**Note on Packages:**  \n",
    "All packages used here are standard sklearn/scipy components introduced in CMT122 labs. No external packages beyond cMyse materials are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn: Data splitting and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Scikit-learn: Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Scikit-learn: Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Scipy: Sparse matrix operations\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data (if needed)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Exploration <a id='section3'></a>\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "The 20_Newsgroups dataset contains news articles from 6 different categories. I load it as a CSV file with two columns:\n",
    "- `text`: Raw article content (may include headers, metadata)\n",
    "- `label`: Category label (class-1 through class-6)\n",
    "\n",
    "**Expected Structure:**\n",
    "```\n",
    "text,label\n",
    "\"From: user@domain.com Subject: ...\",class-1\n",
    "\"From: another@email.com ...\",class-2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET LOADING\n",
      "======================================================================\n",
      "\n",
      "✓ Dataset loaded successfully\n",
      "\n",
      "Dataset shape: (3416, 2)\n",
      "  - Total samples: 3,416\n",
      "  - Columns: ['text', 'label']\n",
      "\n",
      "Missing values:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "======================================================================\n",
      "CLASS DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "label\n",
      "class-1    480\n",
      "class-2    584\n",
      "class-3    591\n",
      "class-4    590\n",
      "class-5    578\n",
      "class-6    593\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of classes: 6\n",
      "\n",
      "Class balance:\n",
      "  class-1:  480 samples (14.05%)\n",
      "  class-2:  584 samples (17.10%)\n",
      "  class-3:  591 samples (17.30%)\n",
      "  class-4:  590 samples (17.27%)\n",
      "  class-5:  578 samples (16.92%)\n",
      "  class-6:  593 samples (17.36%)\n",
      "\n",
      "Imbalance ratio: 1.24x\n",
      "  → Classes are relatively balanced ✓\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Update this path to match yMy file location\n",
    "data_path = '/home/samidunix/projects/CMT122/20_Newsgroups.csv'\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\n Dataset loaded successfully\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"  - Total samples: {len(df):,}\")\n",
    "print(f\"  - Columns: {list(df.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "\n",
    "# Display class distribution\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "print(f\"\\n{class_counts}\")\n",
    "\n",
    "print(f\"\\nNumber of classes: {df['label'].nunique()}\")\n",
    "print(f\"\\nClass balance:\")\n",
    "for label in class_counts.index:\n",
    "    count = class_counts[label]\n",
    "    percentage = 100 * count / len(df)\n",
    "    print(f\"  {label}: {count:4} samples ({percentage:5.2f}%)\")\n",
    "\n",
    "# Check if balanced\n",
    "min_class = class_counts.min()\n",
    "max_class = class_counts.max()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}x\")\n",
    "if imbalance_ratio < 1.5:\n",
    "    print(\"  → Classes are relatively balanced \")\n",
    "else:\n",
    "    print(\"  → Significant class imbalance (consider class_weight='balanced')\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAMPLE ARTICLES (First 100 characters)\n",
      "======================================================================\n",
      "\n",
      "class-5:\n",
      "  from : matt harrop@magic-bbs.corp.apple.com distribution : na organization : macintosh awareness gro...\n",
      "\n",
      "class-4:\n",
      "  from : dannyb@panix.com ( daniel burstein ) subject : re : ( q ) conner hd specs organization : pani...\n",
      "\n",
      "class-1:\n",
      "  subject : re : contradictions from : kmr4@po.cwru.edu ( keith m. ryan ) organization : case western ...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE ARTICLES (First 100 characters)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label in df['label'].unique()[:3]:\n",
    "    sample = df[df['label'] == label].iloc[0]\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  {sample['text'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing <a id='section4'></a>\n",
    "\n",
    "### Preprocessing Strategy\n",
    "\n",
    "Newsgroup articles contain:\n",
    "- Email headers (From:, Subject:, Organization:)\n",
    "- Email addresses\n",
    "- Special characters and punctuation\n",
    "- Mixed case text\n",
    "\n",
    "**My cleaning approach:**\n",
    "1. **Lowercase conversion**: Treat \"Computer\" and \"computer\" as the same\n",
    "2. **Email removal**: Addresses don't help classification\n",
    "3. **Header removal**: \"From:\", \"Subject:\", etc. are noise\n",
    "4. **Non-alphabetic removal**: Keep only letters and spaces\n",
    "5. **Whitespace normalization**: Collapse multiple spaces\n",
    "\n",
    "**Why this approach?**\n",
    "- Focusing on actual content words\n",
    "- Removes metadata that might cause overfitting\n",
    "- Standardizes text format for vectorization\n",
    "\n",
    "**Trade-off**: Losing some potentially useful information (e.g., header fields) but gain cleaner, more general features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEXT PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Cleaning text... (this may take a moment)\n",
      "✓ Text cleaning complete\n",
      "\n",
      "Dataset size after cleaning: 3,416 samples\n",
      "  (Removed 0 articles shorter than 50 chars)\n",
      "\n",
      "======================================================================\n",
      "BEFORE/AFTER CLEANING EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "[Example 1] Label: class-5\n",
      "BEFORE: from : matt harrop@magic-bbs.corp.apple.com distribution : na organization : mac...\n",
      "AFTER:  from matt distribution na organization macintosh awareness group in canada subje...\n",
      "\n",
      "[Example 101] Label: class-6\n",
      "BEFORE: from : donaldlf@k9.rose-hulman.edu ( leslie f. donaldson ) subject : problems us...\n",
      "AFTER:  from leslie f donaldson subject problems using graphic context with athena widge...\n",
      "\n",
      "[Example 201] Label: class-6\n",
      "BEFORE: from : raney@teal.csn.org ( scott raney ) subject : re : hypercard for unix nntp...\n",
      "AFTER:  from scott raney subject re hypercard for unix nntp posting host teal csn org or...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def clean_newsgroup_text(text):\n",
    "    \"\"\"\n",
    "    Clean newsgroup article text for classification.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw article text with headers and metadata\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text containing only lowercase alphabetic words\n",
    "    \n",
    "    Example:\n",
    "        Input:  \"From: user@email.com\\nSubject: Computer Science\\nThis is great!\"\n",
    "        Output: \"computer science this is great\"\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Step 1: Remove email addresses\n",
    "    # Pattern \\S+@\\S+ matches user@domain.com\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    \n",
    "    # Step 2: Remove common header fields\n",
    "    # These appear in newsgroup posts but don't help classification\n",
    "    header_pattern = r'(from|subject|organization|lines|distribution|reply-to):'\n",
    "    text = re.sub(header_pattern, ' ', text)\n",
    "    \n",
    "    # Step 3: Keep only letters and spaces\n",
    "    # [^a-z\\s] means \"not a letter or space\"\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Step 4: Collapse multiple spaces into one\n",
    "    # \\s+ matches one or more whitespace characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Step 5: Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Apply cleaning function to all articles\n",
    "print(\"\\nCleaning text...\")\n",
    "df['cleaned_text'] = df['text'].apply(clean_newsgroup_text)\n",
    "\n",
    "# Filter out very short texts (likely empty after cleaning)\n",
    "# Threshold: 50 characters ensures we have meaningful content\n",
    "min_length = 50\n",
    "df = df[df['cleaned_text'].str.len() > min_length].reset_index(drop=True)\n",
    "\n",
    "print(f\" Text cleaning complete\")\n",
    "print(f\"\\nDataset size after cleaning: {len(df):,} samples\")\n",
    "print(f\"  (Removed {len(pd.read_csv(data_path)) - len(df)} articles shorter than {min_length} chars)\")\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEFORE/AFTER CLEANING EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in [0, 100, 200]:\n",
    "    if idx < len(df):\n",
    "        print(f\"\\n[Example {idx+1}] Label: {df.iloc[idx]['label']}\")\n",
    "        print(f\"BEFORE: {df.iloc[idx]['text'][:80]}...\")\n",
    "        print(f\"AFTER:  {df.iloc[idx]['cleaned_text'][:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Partitioning (Train/Dev/Test) <a id='section5'></a>\n",
    "\n",
    "### Evaluation Protocol: 60/20/20 Split\n",
    "\n",
    "**Three-way split strategy:**\n",
    "1. **Training set (60%)**: Used to fit model parameters\n",
    "2. **Development set (20%)**: Used for hyperparameter tuning\n",
    "3. **Test set (20%)**: Used only for final evaluation\n",
    "\n",
    "**Why this approach?**\n",
    "- **Training set**: Large enough to learn patterns (60% ≈ 2,049 samples)\n",
    "- **Development set**: Allows fair comparison of feature/model choices\n",
    "- **Test set**: Provides unbiased performance estimate (never seen during development)\n",
    "\n",
    "**Alternative approaches considered:**\n",
    "- **80/20 split**: No development set → can't tune hyperparameters fairly\n",
    "- **Cross-validation**: More robust but computationally expensive\n",
    "- **50/25/25 split**: Less training data → potentially worse models\n",
    "\n",
    "**Stratification:**  \n",
    " Utilises stratified sampling to maintain class distribution across all three sets. This ensures each set is representative of the overall population.\n",
    "\n",
    "**Reproducibility:**  \n",
    "`random_state=42` ensures the same split every time (important for fair comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET PARTITIONING\n",
      "======================================================================\n",
      "\n",
      "Total dataset: 3,416 samples\n",
      "\n",
      "Split sizes:\n",
      "  Training:   2,049 samples ( 60.0%)\n",
      "  Development: 683 samples ( 20.0%)\n",
      "  Test:        684 samples ( 20.0%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Class distribution verification:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Class           Train        Dev       Test      Total\n",
      "------------------------------------------------------\n",
      "class-1           288         96         96        480\n",
      "class-2           350        117        117        584\n",
      "class-3           354        118        119        591\n",
      "class-4           354        118        118        590\n",
      "class-5           347        116        115        578\n",
      "class-6           356        118        119        593\n",
      "\n",
      "✓ Stratified split successful - class proportions maintained\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATASET PARTITIONING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First split: 60% train, 40% temp (will become dev + test)\n",
    "df_train, df_temp = train_test_split(\n",
    "    df,\n",
    "    test_size=0.4,           # 40% for temp (dev + test)\n",
    "    random_state=42,          # Reproducibility\n",
    "    stratify=df['label']      # Maintain class distribution\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50% dev, 50% test\n",
    "# This gives the 20% dev and 20% test of original dataset\n",
    "df_dev, df_test = train_test_split(\n",
    "    df_temp,\n",
    "    test_size=0.5,            # 50% of temp = 20% of original\n",
    "    random_state=42,\n",
    "    stratify=df_temp['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal dataset: {len(df):,} samples\")\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Training:   {len(df_train):,} samples ({100*len(df_train)/len(df):5.1f}%)\")\n",
    "print(f\"  Development: {len(df_dev):,} samples ({100*len(df_dev)/len(df):5.1f}%)\")\n",
    "print(f\"  Test:        {len(df_test):,} samples ({100*len(df_test)/len(df):5.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Class distribution verification:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\n{'Class':<10} {'Train':>10} {'Dev':>10} {'Test':>10} {'Total':>10}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "for label in sorted(df['label'].unique()):\n",
    "    train_count = (df_train['label'] == label).sum()\n",
    "    dev_count = (df_dev['label'] == label).sum()\n",
    "    test_count = (df_test['label'] == label).sum()\n",
    "    total = train_count + dev_count + test_count\n",
    "    print(f\"{label:<10} {train_count:>10} {dev_count:>10} {test_count:>10} {total:>10}\")\n",
    "\n",
    "print(\"\\n Stratified split successful - class proportions maintained\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Labels for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Labels extracted:\n",
      "  Training labels: (2049,)\n",
      "  Development labels: (683,)\n",
      "  Test labels: (684,)\n",
      "\n",
      "  Unique classes: ['class-1', 'class-2', 'class-3', 'class-4', 'class-5', 'class-6']\n"
     ]
    }
   ],
   "source": [
    "# Extract labels as numpy arrays\n",
    "y_train = df_train['label'].values\n",
    "y_dev = df_dev['label'].values\n",
    "y_test = df_test['label'].values\n",
    "\n",
    "print(\" Labels extracted:\")\n",
    "print(f\"  Training labels: {y_train.shape}\")\n",
    "print(f\"  Development labels: {y_dev.shape}\")\n",
    "print(f\"  Test labels: {y_test.shape}\")\n",
    "print(f\"\\n  Unique classes: {sorted(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering <a id='section6'></a>\n",
    "\n",
    "### Feature Engineering Overview\n",
    "\n",
    "I extract **four distinct features** from the text:\n",
    "\n",
    "1. **Feature 1: TF-IDF (Primary)** - Word frequency information\n",
    "2. **Feature 2: Word Count** - Document length\n",
    "3. **Feature 3: Average Word Length** - Vocabulary complexity\n",
    "4. **Feature 4: Lexical Diversity** - Vocabulary richness\n",
    "\n",
    "These features capture different aspects of text:\n",
    "- **TF-IDF**: *What* words are used (content)\n",
    "- **Word Count**: *How much* text (length)\n",
    "- **Avg Word Length**: *How complex* vocabulary (sophistication)\n",
    "- **Lexical Diversity**: *How varied* vocabulary (richness)\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1. Feature 1: TF-IDF Vectorization <a id='section6_1'></a>\n",
    "\n",
    "#### What is TF-IDF?\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) converts text to numerical vectors.\n",
    "\n",
    "**Formula:**  \n",
    "TF-IDF(word, doc) = TF(word, doc) × IDF(word)\n",
    "\n",
    "Where:\n",
    "- **TF (Term Frequency)**: How often word appears in document\n",
    "- **IDF (Inverse Document Frequency)**: log(total docs / docs containing word)\n",
    "\n",
    "**Effect:**\n",
    "- Common words (\"the\", \"is\") → Low TF-IDF (appear in many documents)\n",
    "- Distinctive words (\"graphics\", \"hockey\") → High TF-IDF (appear in few documents)\n",
    "\n",
    "#### Parameter Choices\n",
    "\n",
    "1. **`max_features=20000`**: Vocabulary size\n",
    "   - **Why?** Captures diverse newsgroup terminology\n",
    "   - **Trade-off**: More features = more memory, but better coverage\n",
    "\n",
    "2. **`ngram_range=(1, 2)`**: Unigrams and bigrams\n",
    "   - **Unigrams**: \"computer\", \"science\"\n",
    "   - **Bigrams**: \"computer science\", \"machine learning\"\n",
    "   - **Why?** Bigrams capture multi-word concepts\n",
    "\n",
    "3. **`stop_words='english'`**: Remove common words\n",
    "   - Filters: \"the\", \"is\", \"at\", \"on\", etc.\n",
    "   - **Why?** These don't distinguish between categories\n",
    "\n",
    "4. **`min_df=3`**: Minimum document frequency\n",
    "   - Word must appear in at least 3 documents\n",
    "   - **Why?** Removes typos and extremely rare words\n",
    "\n",
    "5. **`max_df=0.75`**: Maximum document frequency\n",
    "   - Word must appear in at most 75% of documents\n",
    "   - **Why?** Removes overly common words\n",
    "\n",
    "6. **`sublinear_tf=True`**: Logarithmic term frequency\n",
    "   - Uses 1 + log(TF) instead of raw TF\n",
    "   - **Why?** Reduces impact of very frequent words within a document\n",
    "\n",
    "#### Why TF-IDF for Text Classification?\n",
    "\n",
    " **Advantages:**\n",
    "- Emphasizes distinctive words\n",
    "- Handles variable-length documents\n",
    "- Sparse representation (memory efficient)\n",
    "- Works well with linear models\n",
    "\n",
    " **Limitations:**\n",
    "- Ignores word order (\"not good\" vs \"good not\")\n",
    "- Treats words independently\n",
    "- No semantic understanding (\"car\" ≠ \"automobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE 1: TF-IDF VECTORIZATION\n",
      "======================================================================\n",
      "\n",
      "Extracting TF-IDF features...\n",
      "  Parameters:\n",
      "    - max_features: 20,000\n",
      "    - ngram_range: (1, 2)\n",
      "    - min_df: 3\n",
      "    - max_df: 0.75\n",
      "\n",
      "✓ TF-IDF extraction complete\n",
      "\n",
      "  Training matrix: (2049, 19226)\n",
      "    - Samples: 2,049\n",
      "    - Features: 19,226\n",
      "    - Sparsity: 99.49%\n",
      "    - Non-zero elements: 199,608\n",
      "\n",
      "  Dev matrix: (683, 19226)\n",
      "  Test matrix: (684, 19226)\n",
      "\n",
      "  Sample features (first 20):\n",
      "    ['aa', 'aaron', 'ab', 'abandon', 'abc', 'abilities', 'ability', 'ability use', 'able', 'able approach', 'able boot', 'able handle', 'able help', 'able locate', 'able run', 'able tell', 'able telnet', 'able unexpected', 'able use', 'able work']\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE 1: TF-IDF VECTORIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize TF-IDF vectorizer with optimized parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,       # Top 20,000 most important terms\n",
    "    ngram_range=(1, 2),       # Unigrams and bigrams\n",
    "    stop_words='english',     # Remove English stopwords\n",
    "    min_df=3,                 # Term must appear in ≥3 documents\n",
    "    max_df=0.75,              # Term must appear in ≤75% of documents\n",
    "    sublinear_tf=True         # Use log scaling for term frequency\n",
    ")\n",
    "\n",
    "print(\"\\nExtracting TF-IDF features...\")\n",
    "print(f\"  Parameters:\")\n",
    "print(f\"    - max_features: 20,000\")\n",
    "print(f\"    - ngram_range: (1, 2)\")\n",
    "print(f\"    - min_df: 3\")\n",
    "print(f\"    - max_df: 0.75\")\n",
    "\n",
    "# fit_transform on training only\n",
    "# This learns vocabulary from training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['cleaned_text'])\n",
    "\n",
    "# transform (not fit_transform) on dev and test\n",
    "# This uses vocabulary learned from training set\n",
    "# Prevents data leakage from dev/test into training\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(df_dev['cleaned_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['cleaned_text'])\n",
    "\n",
    "print(f\"\\n TF-IDF extraction complete\")\n",
    "print(f\"\\n  Training matrix: {X_train_tfidf.shape}\")\n",
    "print(f\"    - Samples: {X_train_tfidf.shape[0]:,}\")\n",
    "print(f\"    - Features: {X_train_tfidf.shape[1]:,}\")\n",
    "print(f\"    - Sparsity: {100 * (1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])):.2f}%\")\n",
    "print(f\"    - Non-zero elements: {X_train_tfidf.nnz:,}\")\n",
    "\n",
    "print(f\"\\n  Dev matrix: {X_dev_tfidf.shape}\")\n",
    "print(f\"  Test matrix: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Show sample features\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\n  Sample features (first 20):\")\n",
    "print(f\"    {list(feature_names[:20])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Features 2-4: Statistical Text Features <a id='section6_2'></a>\n",
    "\n",
    "#### Why Statistical Features?\n",
    "\n",
    "While TF-IDF captures *what* words are used, statistical features capture document-level properties:\n",
    "- **Different signal**: Complements TF-IDF with meta-information\n",
    "- **Robust**: Less prone to overfitting than individual word features\n",
    "- **Interpretable**: Easy to understand and explain\n",
    "\n",
    "#### Feature Descriptions\n",
    "\n",
    "**Feature 2: Word Count**\n",
    "- **Definition**: Total number of words in document\n",
    "- **Formula**: `len(text.split())`\n",
    "- **Intuition**: Technical posts might be longer than casual discussions\n",
    "- **Example**: \"hello world\" → 2, \"this is a test\" → 4\n",
    "\n",
    "**Feature 3: Average Word Length**\n",
    "- **Definition**: Mean number of characters per word\n",
    "- **Formula**: `sum(len(word) for word in words) / len(words)`\n",
    "- **Intuition**: Technical topics use longer words (\"algorithm\" vs \"hi\")\n",
    "- **Example**: \"hi bye\" → 2.5, \"computer science\" → 7.5\n",
    "\n",
    "**Feature 4: Lexical Diversity (Type-Token Ratio)**\n",
    "- **Definition**: Ratio of unique words to total words\n",
    "- **Formula**: `len(set(words)) / len(words)`\n",
    "- **Intuition**: Repetitive posts have lower diversity\n",
    "- **Range**: [0, 1] where 1 = all words unique\n",
    "- **Example**: \"the the the cat\" → 0.25, \"a quick brown fox\" → 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURES 2-4: STATISTICAL TEXT FEATURES\n",
      "======================================================================\n",
      "\n",
      "Extracting statistical features...\n",
      "\n",
      "✓ Statistical features extracted\n",
      "\n",
      "  Feature shapes:\n",
      "    Training: (2049, 3)\n",
      "    Dev:      (683, 3)\n",
      "    Test:     (684, 3)\n",
      "\n",
      "  Feature statistics (training set):\n",
      "\n",
      "  Feature                     Min       Mean        Max        Std\n",
      "  ----------------------------------------------------------------\n",
      "  Word Count                11.00     301.86   15252.00    1070.77\n",
      "  Avg Word Length            1.61       4.49       6.60       0.43\n",
      "  Lexical Diversity          0.01       0.67       1.00       0.13\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def extract_statistical_features(text_series):\n",
    "    \"\"\"\n",
    "    Extract three statistical features from a series of text documents.\n",
    "    \n",
    "    Args:\n",
    "        text_series (pd.Series): Series of text strings\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (n_samples, 3) containing:\n",
    "            - Column 0: Word count\n",
    "            - Column 1: Average word length\n",
    "            - Column 2: Lexical diversity (unique/total ratio)\n",
    "            \n",
    "    Note:\n",
    "        Handles edge cases:\n",
    "        - Empty text → [0, 0, 0]\n",
    "        - Single word → word_count=1, diversity=1.0\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for text in text_series:\n",
    "        # Tokenize by splitting on whitespace\n",
    "        words = text.split()\n",
    "        \n",
    "        # Feature 2: Word count\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Feature 3: Average word length\n",
    "        # Handle empty text to avoid division by zero\n",
    "        if words:\n",
    "            avg_word_length = np.mean([len(w) for w in words])\n",
    "        else:\n",
    "            avg_word_length = 0\n",
    "        \n",
    "        # Feature 4: Lexical diversity (type-token ratio)\n",
    "        # Ratio of unique words to total words\n",
    "        if words:\n",
    "            unique_words = len(set(words))\n",
    "            lexical_diversity = unique_words / len(words)\n",
    "        else:\n",
    "            lexical_diversity = 0\n",
    "        \n",
    "        features.append([word_count, avg_word_length, lexical_diversity])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURES 2-4: STATISTICAL TEXT FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nExtracting statistical features...\")\n",
    "\n",
    "# Extract features for all three sets\n",
    "X_train_stats = extract_statistical_features(df_train['cleaned_text'])\n",
    "X_dev_stats = extract_statistical_features(df_dev['cleaned_text'])\n",
    "X_test_stats = extract_statistical_features(df_test['cleaned_text'])\n",
    "\n",
    "print(f\"\\n Statistical features extracted\")\n",
    "print(f\"\\n  Feature shapes:\")\n",
    "print(f\"    Training: {X_train_stats.shape}\")\n",
    "print(f\"    Dev:      {X_dev_stats.shape}\")\n",
    "print(f\"    Test:     {X_test_stats.shape}\")\n",
    "\n",
    "# Display statistics about the features\n",
    "print(f\"\\n  Feature statistics (training set):\")\n",
    "feature_names_stats = ['Word Count', 'Avg Word Length', 'Lexical Diversity']\n",
    "print(f\"\\n  {'Feature':<20} {'Min':>10} {'Mean':>10} {'Max':>10} {'Std':>10}\")\n",
    "print(\"  \" + \"-\" * 64)\n",
    "for i, name in enumerate(feature_names_stats):\n",
    "    col = X_train_stats[:, i]\n",
    "    print(f\"  {name:<20} {col.min():>10.2f} {col.mean():>10.2f} {col.max():>10.2f} {col.std():>10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling for Statistical Features\n",
    "\n",
    "**Why Scale?**\n",
    "\n",
    "My three statistical features have very different ranges:\n",
    "- Word Count: [0, ~500]\n",
    "- Avg Word Length: [0, ~10]\n",
    "- Lexical Diversity: [0, 1]\n",
    "\n",
    "**Problem without scaling:**\n",
    "- Chi-squared feature selection **requires non-negative features**\n",
    "- Features with larger ranges dominate distance calculations\n",
    "- Some models (SVMs) are sensitive to feature scale\n",
    "\n",
    "**My solution: MinMaxScaler**\n",
    "- **Formula**: `(x - min) / (max - min)`\n",
    "- **Result**: All features scaled to [0, 1] range\n",
    "- **Advantage**: Preserves zero values (which is important for sparse features)\n",
    "\n",
    "**Alternative: StandardScaler**\n",
    "- Formula: `(x - mean) / std`\n",
    "- Result: Mean=0, Std=1\n",
    "- **Why not used**: Can produce negative values → incompatible with chi-squared\n",
    "\n",
    "Fit scaler on training set only, then transform all sets\n",
    "- **fit()** on training: Learns min/max values\n",
    "- **transform()** on dev/test: Applies training min/max\n",
    "- This prevents data leakage from dev/test into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SCALING\n",
      "======================================================================\n",
      "\n",
      "Scaling statistical features to [0, 1] range...\n",
      "  Method: MinMaxScaler\n",
      "  Reason: Required for chi-squared feature selection\n",
      "\n",
      "✓ Scaling complete\n",
      "\n",
      "  Scaled feature ranges (training):\n",
      "\n",
      "  Feature                     Min       Mean        Max\n",
      "  ----------------------------------------------\n",
      "  Word Count               0.0000     0.0191     1.0000\n",
      "  Avg Word Length          0.0000     0.5763     1.0000\n",
      "  Lexical Diversity        0.0000     0.6710     1.0000\n",
      "\n",
      "  ✓ All features now in [0, 1] range\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nScaling statistical features to [0, 1] range...\")\n",
    "print(\"  Method: MinMaxScaler\")\n",
    "print(\"  Reason: Required for chi-squared feature selection\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# CRITICAL: Fit on training data only\n",
    "X_train_stats_scaled = scaler.fit_transform(X_train_stats)\n",
    "\n",
    "# Transform dev and test using training statistics\n",
    "X_dev_stats_scaled = scaler.transform(X_dev_stats)\n",
    "X_test_stats_scaled = scaler.transform(X_test_stats)\n",
    "\n",
    "print(f\"\\n Scaling complete\")\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\n  Scaled feature ranges (training):\")\n",
    "print(f\"\\n  {'Feature':<20} {'Min':>10} {'Mean':>10} {'Max':>10}\")\n",
    "print(\"  \" + \"-\" * 46)\n",
    "for i, name in enumerate(feature_names_stats):\n",
    "    col = X_train_stats_scaled[:, i]\n",
    "    print(f\"  {name:<20} {col.min():>10.4f} {col.mean():>10.4f} {col.max():>10.4f}\")\n",
    "\n",
    "print(\"\\n   All features now in [0, 1] range\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Feature Combination <a id='section6_3'></a>\n",
    "\n",
    "#### Combining Sparse and Dense Features\n",
    "\n",
    "Now have two feature types:\n",
    "1. **TF-IDF features**: Sparse matrix (many zeros) - 19,226 features\n",
    "2. **Statistical features**: Dense array (all values non-zero) - 3 features\n",
    "\n",
    "**Problem**: How to combine sparse and dense data efficiently?\n",
    "\n",
    "**Solution**: Use `scipy.sparse.hstack()`\n",
    "- Horizontally stacks (concatenates) feature matrices\n",
    "- Maintains sparse format for efficiency\n",
    "- Result: Single sparse matrix with all features\n",
    "\n",
    "**Process**:\n",
    "1. Convert dense statistical features to sparse format (`csr_matrix`)\n",
    "2. Horizontally stack: [TF-IDF features | Statistical features]\n",
    "3. Result: Combined sparse matrix\n",
    "\n",
    "**Memory efficiency**:\n",
    "- Sparse format stores only non-zero values\n",
    "- TF-IDF is ~98% sparse → huge memory savings\n",
    "- Critical for large feature spaces (19,229 features)\n",
    "\n",
    "**Final feature vector for each document**:\n",
    "```\n",
    "[word_1_tfidf, word_2_tfidf, ..., word_19226_tfidf, word_count, avg_word_len, lexical_div]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE COMBINATION\n",
      "======================================================================\n",
      "\n",
      "Combining TF-IDF and statistical features...\n",
      "\n",
      "  Converting statistical features to sparse format...\n",
      "    Training stats: (2049, 3) (dense) → (2049, 3) (sparse)\n",
      "\n",
      "✓ Feature combination complete\n",
      "\n",
      "  Combined feature matrix:\n",
      "    Training:   (2049, 19229)\n",
      "    Dev:        (683, 19229)\n",
      "    Test:       (684, 19229)\n",
      "\n",
      "  Feature breakdown:\n",
      "    TF-IDF features:      19,226\n",
      "    Statistical features:      3\n",
      "    -----------------------------------\n",
      "    Total features:       19,229\n",
      "\n",
      "  Memory efficiency:\n",
      "    Sparsity: 99.48%\n",
      "    Non-zero elements: 205,752 / 39,400,221\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE COMBINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nCombining TF-IDF and statistical features...\")\n",
    "\n",
    "# Convert dense statistical features to sparse format\n",
    "# This is memory-efficient and allows combining with TF-IDF\n",
    "X_train_stats_sparse = csr_matrix(X_train_stats_scaled)\n",
    "X_dev_stats_sparse = csr_matrix(X_dev_stats_scaled)\n",
    "X_test_stats_sparse = csr_matrix(X_test_stats_scaled)\n",
    "\n",
    "print(f\"\\n  Converting statistical features to sparse format...\")\n",
    "print(f\"    Training stats: {X_train_stats_scaled.shape} (dense) → {X_train_stats_sparse.shape} (sparse)\")\n",
    "\n",
    "# Horizontally stack features\n",
    "# Result: [TF-IDF features | Statistical features]\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_stats_sparse])\n",
    "X_dev_combined = hstack([X_dev_tfidf, X_dev_stats_sparse])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_stats_sparse])\n",
    "\n",
    "print(f\"\\n Feature combination complete\")\n",
    "\n",
    "print(f\"\\n  Combined feature matrix:\")\n",
    "print(f\"    Training:   {X_train_combined.shape}\")\n",
    "print(f\"    Dev:        {X_dev_combined.shape}\")\n",
    "print(f\"    Test:       {X_test_combined.shape}\")\n",
    "\n",
    "print(f\"\\n  Feature breakdown:\")\n",
    "print(f\"    TF-IDF features:      {X_train_tfidf.shape[1]:,}\")\n",
    "print(f\"    Statistical features:      {X_train_stats_sparse.shape[1]}\")\n",
    "print(f\"    \" + \"-\" * 35)\n",
    "print(f\"    Total features:       {X_train_combined.shape[1]:,}\")\n",
    "\n",
    "# Memory efficiency check\n",
    "sparsity = 100 * (1 - X_train_combined.nnz / (X_train_combined.shape[0] * X_train_combined.shape[1]))\n",
    "print(f\"\\n  Memory efficiency:\")\n",
    "print(f\"    Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"    Non-zero elements: {X_train_combined.nnz:,} / {X_train_combined.shape[0] * X_train_combined.shape[1]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection (Development Set) <a id='section7'></a>\n",
    "\n",
    "### Why Feature Selection?\n",
    "\n",
    "Currently have 19,229 features - this creates several problems:\n",
    "1. **Curse of dimensionality**: High dimensions → sparse data → poor generalization\n",
    "2. **Computational cost**: More features = slower training\n",
    "3. **Overfitting risk**: Model may learn noise instead of signal\n",
    "4. **Noise**: Many features are irrelevant or redundant\n",
    "\n",
    "**Solution**: Use feature selection to keep only the most informative features.\n",
    "\n",
    "### Chi-Squared (χ²) Feature Selection\n",
    "\n",
    "**What it measures:**  \n",
    "Chi-squared tests the independence between each feature and the class labels.\n",
    "\n",
    "**Formula:**  \n",
    "χ² = Σ (Observed - Expected)² / Expected\n",
    "\n",
    "**Intuition:**\n",
    "- **High χ²**: Feature strongly associated with certain classes\n",
    "- **Low χ²**: Feature appears randomly across classes\n",
    "\n",
    "**Example:**\n",
    "- Word \"hockey\" appears frequently in sports class → High χ²\n",
    "- Word \"the\" appears equally in all classes → Low χ²\n",
    "\n",
    "**Why chi-squared for text?**\n",
    "- Fast computation\n",
    "- Works well with sparse data\n",
    "- Interpretable results\n",
    "- Commonly used for text classification\n",
    "\n",
    "**Requirement**: Features must be non-negative (hence My MinMaxScaler choice)\n",
    "\n",
    "### Development Set Experiments\n",
    "\n",
    "Tested on different values of k (number of features to keep):\n",
    "- **k=1,000**: Very aggressive reduction (95% features removed)\n",
    "- **k=5,000**: Moderate reduction (74% features removed)\n",
    "- **k=10,000**: Conservative reduction (48% features removed)\n",
    "- **k=15,000**: Minimal reduction (22% features removed)\n",
    "- **k=all (19,229)**: No reduction (baseline)\n",
    "\n",
    "**Why use development set?**\n",
    "- Simulates test set performance\n",
    "- Allows fair comparison of different k values\n",
    "- Prevents overfitting to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SELECTION: DEVELOPMENT SET EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "Testing different feature set sizes...\n",
      "Method: Chi-squared (χ²) statistical test\n",
      "Model: LinearSVC (for speed)\n",
      "\n",
      "Total features available: 19,229\n",
      "\n",
      "Testing k values: ['1,000', '5,000', '10,000', '15,000', '19,229']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "k=1,000 (5.2% of features)\n",
      "  Reduction:  94.8% features removed\n",
      "  Dev Accuracy: 0.8463 (84.63%)\n",
      "\n",
      "k=5,000 (26.0% of features)\n",
      "  Reduction:  74.0% features removed\n",
      "  Dev Accuracy: 0.8917 (89.17%)\n",
      "\n",
      "k=10,000 (52.0% of features)\n",
      "  Reduction:  48.0% features removed\n",
      "  Dev Accuracy: 0.9004 (90.04%)\n",
      "\n",
      "k=15,000 (78.0% of features)\n",
      "  Reduction:  22.0% features removed\n",
      "  Dev Accuracy: 0.9034 (90.34%)\n",
      "\n",
      "k=19,229 (all features - no selection)\n",
      "  Reduction:   0.0% features removed\n",
      "  Dev Accuracy: 0.8975 (89.75%)\n",
      "\n",
      "======================================================================\n",
      "FEATURE SELECTION RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "k              % Kept    Dev Accuracy Status\n",
      "------------------------------------------------------------\n",
      "1,000            5.2%         0.8463\n",
      "5,000           26.0%         0.8917\n",
      "10,000          52.0%         0.9004\n",
      "15,000          78.0%         0.9034 ★ BEST\n",
      "19,229         100.0%         0.8975\n",
      "\n",
      "======================================================================\n",
      "✓ BEST k: 15,000 features\n",
      "  Dev Accuracy: 0.9034 (90.34%)\n",
      "  Reduction: 22.0% features removed\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SELECTION: DEVELOPMENT SET EXPERIMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTesting different feature set sizes...\")\n",
    "print(\"Method: Chi-squared (χ²) statistical test\")\n",
    "print(\"Model: LinearSVC (for speed)\")\n",
    "\n",
    "# Test different k values\n",
    "# Note: Adapted to My feature space size (19,229)\n",
    "k_values = [1000, 5000, 10000, 15000, X_train_combined.shape[1]]\n",
    "dev_results = []\n",
    "\n",
    "print(f\"\\nTotal features available: {X_train_combined.shape[1]:,}\")\n",
    "print(f\"\\nTesting k values: {[f'{k:,}' for k in k_values]}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "for k in k_values:\n",
    "    # Handle case where k >= total features\n",
    "    if k >= X_train_combined.shape[1]:\n",
    "        k_actual = X_train_combined.shape[1]\n",
    "        X_train_selected = X_train_combined\n",
    "        X_dev_selected = X_dev_combined\n",
    "        print(f\"\\nk={k_actual:,} (all features - no selection)\")\n",
    "    else:\n",
    "        k_actual = k\n",
    "        print(f\"\\nk={k_actual:,} ({100*k_actual/X_train_combined.shape[1]:.1f}% of features)\")\n",
    "        \n",
    "        # Perform feature selection\n",
    "        # fit() on training: Learn which features are most informative\n",
    "        # transform() on dev: Apply same selection\n",
    "        selector = SelectKBest(chi2, k=k_actual)\n",
    "        X_train_selected = selector.fit_transform(X_train_combined, y_train)\n",
    "        X_dev_selected = selector.transform(X_dev_combined)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    # Using LinearSVC for speed (feature selection experiments take time)\n",
    "    model = LinearSVC(C=1.0, max_iter=2000, random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluate on development set\n",
    "    y_pred_dev = model.predict(X_dev_selected)\n",
    "    acc = accuracy_score(y_dev, y_pred_dev)\n",
    "    \n",
    "    # Store results\n",
    "    dev_results.append({\n",
    "        'k': k_actual,\n",
    "        'dev_accuracy': acc,\n",
    "        'reduction': 100 * (1 - k_actual / X_train_combined.shape[1])\n",
    "    })\n",
    "    \n",
    "    print(f\"  Reduction: {dev_results[-1]['reduction']:5.1f}% features removed\")\n",
    "    print(f\"  Dev Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# Find best k\n",
    "best_result = max(dev_results, key=lambda x: x['dev_accuracy'])\n",
    "best_k = best_result['k']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE SELECTION RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'k':<10} {'% Kept':>10} {'Dev Accuracy':>15} {'Status'}\")\n",
    "print(\"-\" * 60)\n",
    "for result in dev_results:\n",
    "    pct_kept = 100 * result['k'] / X_train_combined.shape[1]\n",
    "    marker = \" ★ BEST\" if result['k'] == best_k else \"\"\n",
    "    print(f\"{result['k']:<10,} {pct_kept:>9.1f}% {result['dev_accuracy']:>14.4f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\" BEST k: {best_k:,} features\")\n",
    "print(f\"  Dev Accuracy: {best_result['dev_accuracy']:.4f} ({best_result['dev_accuracy']*100:.2f}%)\")\n",
    "print(f\"  Reduction: {best_result['reduction']:.1f}% features removed\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Best Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "APPLYING BEST FEATURE SELECTION\n",
      "======================================================================\n",
      "\n",
      "Selecting top 15,000 features using chi-squared...\n",
      "✓ Feature selection applied\n",
      "\n",
      "Final feature matrix shapes:\n",
      "  Training:   (2049, 15000)\n",
      "  Dev:        (683, 15000)\n",
      "  Test:       (684, 15000)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"APPLYING BEST FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if best_k >= X_train_combined.shape[1]:\n",
    "    # Use all features\n",
    "    print(f\"\\nUsing all {X_train_combined.shape[1]:,} features (no selection needed)\")\n",
    "    X_train_final = X_train_combined\n",
    "    X_dev_final = X_dev_combined\n",
    "    X_test_final = X_test_combined\n",
    "else:\n",
    "    # Apply feature selection with best k\n",
    "    print(f\"\\nSelecting top {best_k:,} features using chi-squared...\")\n",
    "    final_selector = SelectKBest(chi2, k=best_k)\n",
    "    \n",
    "    # Fit on training and transform all sets\n",
    "    X_train_final = final_selector.fit_transform(X_train_combined, y_train)\n",
    "    X_dev_final = final_selector.transform(X_dev_combined)\n",
    "    X_test_final = final_selector.transform(X_test_combined)\n",
    "    \n",
    "    print(f\" Feature selection applied\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shapes:\")\n",
    "print(f\"  Training:   {X_train_final.shape}\")\n",
    "print(f\"  Dev:        {X_dev_final.shape}\")\n",
    "print(f\"  Test:       {X_test_final.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection (Development Set) <a id='section8'></a>\n",
    "\n",
    "### Model Comparison Strategy\n",
    "\n",
    "Comparing multiple models to find the best classifier for My task:\n",
    "\n",
    "**Models Tested:**\n",
    "\n",
    "1. **LinearSVC (C=1.0)** - Linear Support Vector Machine\n",
    "\n",
    "2. **LinearSVC (C=0.5)** - LinearSVC with stronger regularization\n",
    "\n",
    "3. **Logistic Regression** - Probabilistic linear classifier\n",
    "\n",
    "4. **Multinomial Naive Bayes** - Probabilistic classifier\n",
    "\n",
    "### Why These Models?\n",
    "\n",
    "All are **linear models** suited for high-dimensional text:\n",
    "- Text features are often linearly separable\n",
    "- Non-linear models (trees, neural nets) require more data\n",
    "- Linear models are interpretable (can examine feature weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL SELECTION: DEVELOPMENT SET EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "Testing multiple classification models...\n",
      "Feature dimensions: 15,000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[1/4] LinearSVC (C=1.0)\n",
      "  Description: Linear SVM with standard regularization\n",
      "  Dev Accuracy: 0.9034 (90.34%)\n",
      "\n",
      "[2/4] LinearSVC (C=0.5)\n",
      "  Description: Linear SVM with stronger regularization\n",
      "  Dev Accuracy: 0.9019 (90.19%)\n",
      "\n",
      "[3/4] Logistic Regression\n",
      "  Description: Probabilistic linear classifier\n",
      "  Dev Accuracy: 0.8799 (87.99%)\n",
      "\n",
      "[4/4] Multinomial Naive Bayes\n",
      "  Description: Fast probabilistic classifier\n",
      "  Dev Accuracy: 0.8858 (88.58%)\n",
      "\n",
      "======================================================================\n",
      "MODEL SELECTION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Rank   Model                        Dev Accuracy\n",
      "--------------------------------------------------\n",
      "1      LinearSVC (C=1.0)                 0.9034 ★ BEST\n",
      "2      LinearSVC (C=0.5)                 0.9019\n",
      "3      Multinomial NB                    0.8858\n",
      "4      Logistic Regression               0.8799\n",
      "\n",
      "======================================================================\n",
      "✓ BEST MODEL: LinearSVC (C=1.0)\n",
      "  Dev Accuracy: 0.9034 (90.34%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL SELECTION: DEVELOPMENT SET EXPERIMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTesting multiple classification models...\")\n",
    "print(f\"Feature dimensions: {X_train_final.shape[1]:,}\")\n",
    "print(f\"\\n\" + \"-\" * 70)\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = []\n",
    "\n",
    "# Model 1: LinearSVC with C=1.0\n",
    "print(\"\\n[1/4] LinearSVC (C=1.0)\")\n",
    "print(\"  Description: Linear SVM with standard regularization\")\n",
    "lsvc_10 = LinearSVC(C=1.0, max_iter=2000, random_state=42)\n",
    "lsvc_10.fit(X_train_final, y_train)\n",
    "y_pred_lsvc10 = lsvc_10.predict(X_dev_final)\n",
    "acc_lsvc10 = accuracy_score(y_dev, y_pred_lsvc10)\n",
    "model_results.append({\n",
    "    'name': 'LinearSVC (C=1.0)',\n",
    "    'model': lsvc_10,\n",
    "    'dev_accuracy': acc_lsvc10\n",
    "})\n",
    "print(f\"  Dev Accuracy: {acc_lsvc10:.4f} ({acc_lsvc10*100:.2f}%)\")\n",
    "\n",
    "# Model 2: LinearSVC with C=0.5\n",
    "print(\"\\n[2/4] LinearSVC (C=0.5)\")\n",
    "print(\"  Description: Linear SVM with stronger regularization\")\n",
    "lsvc_05 = LinearSVC(C=0.5, max_iter=2000, random_state=42)\n",
    "lsvc_05.fit(X_train_final, y_train)\n",
    "y_pred_lsvc05 = lsvc_05.predict(X_dev_final)\n",
    "acc_lsvc05 = accuracy_score(y_dev, y_pred_lsvc05)\n",
    "model_results.append({\n",
    "    'name': 'LinearSVC (C=0.5)',\n",
    "    'model': lsvc_05,\n",
    "    'dev_accuracy': acc_lsvc05\n",
    "})\n",
    "print(f\"  Dev Accuracy: {acc_lsvc05:.4f} ({acc_lsvc05*100:.2f}%)\")\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "print(\"\\n[3/4] Logistic Regression\")\n",
    "print(\"  Description: Probabilistic linear classifier\")\n",
    "lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_final, y_train)\n",
    "y_pred_lr = lr.predict(X_dev_final)\n",
    "acc_lr = accuracy_score(y_dev, y_pred_lr)\n",
    "model_results.append({\n",
    "    'name': 'Logistic Regression',\n",
    "    'model': lr,\n",
    "    'dev_accuracy': acc_lr\n",
    "})\n",
    "print(f\"  Dev Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n",
    "\n",
    "# Model 4: Multinomial Naive Bayes\n",
    "print(\"\\n[4/4] Multinomial Naive Bayes\")\n",
    "print(\"  Description: Fast probabilistic classifier\")\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "mnb.fit(X_train_final, y_train)\n",
    "y_pred_mnb = mnb.predict(X_dev_final)\n",
    "acc_mnb = accuracy_score(y_dev, y_pred_mnb)\n",
    "model_results.append({\n",
    "    'name': 'Multinomial NB',\n",
    "    'model': mnb,\n",
    "    'dev_accuracy': acc_mnb\n",
    "})\n",
    "print(f\"  Dev Accuracy: {acc_mnb:.4f} ({acc_mnb*100:.2f}%)\")\n",
    "\n",
    "# Find best model\n",
    "best_model_result = max(model_results, key=lambda x: x['dev_accuracy'])\n",
    "final_model = best_model_result['model']\n",
    "final_model_name = best_model_result['name']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL SELECTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_results = sorted(model_results, key=lambda x: x['dev_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Model':<25} {'Dev Accuracy':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for rank, result in enumerate(sorted_results, 1):\n",
    "    marker = \" ★ BEST\" if rank == 1 else \"\"\n",
    "    print(f\"{rank:<6} {result['name']:<25} {result['dev_accuracy']:>14.4f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\" BEST MODEL: {final_model_name}\")\n",
    "print(f\"  Dev Accuracy: {best_model_result['dev_accuracy']:.4f} ({best_model_result['dev_accuracy']*100:.2f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation (Test Set) <a id='section9'></a>\n",
    "\n",
    "### Final Model Training\n",
    "\n",
    "Now that we've selected:\n",
    "- Best number of features (k from feature selection)\n",
    "- Best model (from model comparison)\n",
    "\n",
    "We train the final model on **training + development** data:\n",
    "\n",
    "**Why train on train+dev?**\n",
    "- Development set was only used for tuning, not training\n",
    "- More training data → better model\n",
    "\n",
    "**Note**: Used `vstack` (vertical stacking) to combine samples:\n",
    "- Adds more ROWS (samples), not columns (features)\n",
    "- Train: 2,049 samples + Dev: 683 samples = 2,732 samples\n",
    "\n",
    "### Test Set Evaluation\n",
    "\n",
    "**Note**: This is the first time the test set is touched!\n",
    "\n",
    "The test set provides an **unbiased estimate** of performance:\n",
    "- Never seen during development\n",
    "- Not used for any decisions\n",
    "- Represents \"real-world\" performance\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "We compute comprehensive metrics:\n",
    "\n",
    "1. **Accuracy**: Overall correctness\n",
    "   - Formula: (Correct predictions) / (Total predictions)\n",
    "\n",
    "2. **Macro-averaged Precision**: Average precision across all classes\n",
    "   - Formula: (Σ Precision per class) / (Number of classes)\n",
    "\n",
    "3. **Macro-averaged Recall**: Average recall across all classes\n",
    "   - Formula: (Σ Recall per class) / (Number of classes)\n",
    "\n",
    "4. **Macro-averaged F1-score**: Harmonic mean of precision and recall\n",
    "   - Formula: 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "**Why macro-averaging?**\n",
    "- Assignment requirement\n",
    "- Treats all classes equally (doesn't favor majority class)\n",
    "- Better for class-imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL MODEL TRAINING (Train + Dev)\n",
      "======================================================================\n",
      "\n",
      "Combining training and development sets...\n",
      "  Training set:     (2049, 15000)\n",
      "  Development set:  (683, 15000)\n",
      "  Combined set:     (2732, 15000)\n",
      "\n",
      "  Labels: 2,732 samples\n",
      "\n",
      "Training final model: LinearSVC (C=1.0)...\n",
      "✓ Final model trained\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION (FINAL PERFORMANCE)\n",
      "======================================================================\n",
      "\n",
      "Evaluating on held-out test set...\n",
      "(First time touching test set!)\n",
      "\n",
      "======================================================================\n",
      "FINAL TEST SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Model: LinearSVC (C=1.0)\n",
      "Features: 15,000\n",
      "Test samples: 684\n",
      "\n",
      "Metric                              Value\n",
      "------------------------------------------\n",
      "Accuracy                           0.8874 (88.74%)\n",
      "Macro-averaged Precision           0.8909\n",
      "Macro-averaged Recall              0.8906\n",
      "Macro-averaged F1-score            0.8906\n",
      "\n",
      "======================================================================\n",
      "✓ SUCCESS: 88.74% exceeds 65% requirement!\n",
      "  Margin above requirement: +23.74 percentage points\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING (Train + Dev)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nCombining training and development sets...\")\n",
    "\n",
    "# Vertically stack training and dev sets\n",
    "# vstack adds more SAMPLES (rows), not features (columns)\n",
    "X_traindev = vstack([X_train_final, X_dev_final])\n",
    "y_traindev = np.concatenate([y_train, y_dev])\n",
    "\n",
    "print(f\"  Training set:     {X_train_final.shape}\")\n",
    "print(f\"  Development set:  {X_dev_final.shape}\")\n",
    "print(f\"  Combined set:     {X_traindev.shape}\")\n",
    "print(f\"\\n  Labels: {len(y_traindev):,} samples\")\n",
    "\n",
    "print(f\"\\nTraining final model: {final_model_name}...\")\n",
    "final_model.fit(X_traindev, y_traindev)\n",
    "print(\" Final model trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST SET EVALUATION (FINAL PERFORMANCE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nEvaluating on held-out test set...\")\n",
    "print(\"(First time touching test set!)\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate all required metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {final_model_name}\")\n",
    "print(f\"Features: {X_test_final.shape[1]:,}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Value':>10}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'Accuracy':<30} {test_accuracy:>10.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"{'Macro-averaged Precision':<30} {test_precision:>10.4f}\")\n",
    "print(f\"{'Macro-averaged Recall':<30} {test_recall:>10.4f}\")\n",
    "print(f\"{'Macro-averaged F1-score':<30} {test_f1:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Check if requirement met\n",
    "if test_accuracy >= 0.65:\n",
    "    print(f\" SUCCESS: {test_accuracy*100:.2f}% exceeds 65% requirement!\")\n",
    "    margin = (test_accuracy - 0.65) * 100\n",
    "    print(f\"  Margin above requirement: +{margin:.2f} percentage points\")\n",
    "else:\n",
    "    print(f\"⚠ WARNING: {test_accuracy*100:.2f}% below 65% requirement\")\n",
    "    shortfall = (0.65 - test_accuracy) * 100\n",
    "    print(f\"  Shortfall: -{shortfall:.2f} percentage points\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Performance Analysis <a id='section10'></a>\n",
    "\n",
    "### Per-Class Performance\n",
    "\n",
    "Understanding which classes the model handles well (and which it struggles with) is crucial for:\n",
    "- Identifying model weaknesses\n",
    "- Guiding future improvements\n",
    "- Understanding dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Model: LinearSVC (C=1.0)\n",
      "Test samples: 684\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class-1     0.9896    0.9896    0.9896        96\n",
      "     class-2     0.8347    0.8632    0.8487       117\n",
      "     class-3     0.8583    0.8655    0.8619       119\n",
      "     class-4     0.8407    0.8051    0.8225       118\n",
      "     class-5     0.9286    0.9043    0.9163       115\n",
      "     class-6     0.8934    0.9160    0.9046       119\n",
      "\n",
      "    accuracy                         0.8874       684\n",
      "   macro avg     0.8909    0.8906    0.8906       684\n",
      "weighted avg     0.8876    0.8874    0.8873       684\n",
      "\n",
      "\n",
      "======================================================================\n",
      "CONFUSION MATRIX\n",
      "======================================================================\n",
      "\n",
      "Rows: True labels, Columns: Predicted labels\n",
      "\n",
      "         class-1  class-2  class-3  class-4  class-5  class-6\n",
      "class-1       95        0        0        1        0        0\n",
      "class-2        0      101        4        5        2        5\n",
      "class-3        0        8      103        3        0        5\n",
      "class-4        0        8        7       95        6        2\n",
      "class-5        0        2        4        4      104        1\n",
      "class-6        1        2        2        5        0      109\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Most Common Misclassifications:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "True Class      Predicted As         Count\n",
      "------------------------------------------\n",
      "class-3         class-2                  8\n",
      "class-4         class-2                  8\n",
      "class-4         class-3                  7\n",
      "class-4         class-5                  6\n",
      "class-2         class-4                  5\n",
      "class-2         class-6                  5\n",
      "class-3         class-6                  5\n",
      "class-6         class-4                  5\n",
      "class-2         class-3                  4\n",
      "class-5         class-3                  4\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {final_model_name}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(classification_report(y_test, y_pred_test, zero_division=0, digits=4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "classes = sorted(np.unique(y_test))\n",
    "\n",
    "# Display as DataFrame for readability\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(\"\\nRows: True labels, Columns: Predicted labels\")\n",
    "print(f\"\\n{cm_df}\")\n",
    "\n",
    "# Identify common misclassifications\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Most Common Misclassifications:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "misclassifications = []\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        if i != j and cm[i, j] > 0:  # Off-diagonal elements\n",
    "            misclassifications.append((\n",
    "                classes[i],  # True class\n",
    "                classes[j],  # Predicted class\n",
    "                cm[i, j]     # Count\n",
    "            ))\n",
    "\n",
    "# Sort by count\n",
    "misclassifications.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"\\n{'True Class':<15} {'Predicted As':<15} {'Count':>10}\")\n",
    "print(\"-\" * 42)\n",
    "for true_class, pred_class, count in misclassifications[:10]:  # Top 10\n",
    "    print(f\"{true_class:<15} {pred_class:<15} {count:>10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
